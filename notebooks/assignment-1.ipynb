{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição\n",
    "\n",
    "Nessa tarefa você vai programar regressão linear simples do zero em python. Para isso, você vai assistir o seguinte vídeo no [youtube](https://www.youtube.com/watch?v=XdM6ER7zTLk) e seguir o passo a passo da implementação. Embora o código esteja disponível no [github](https://github.com/llSourcell/linear_regression_live) do autor. \n",
    "\n",
    "É importante que você digite o código enquanto assiste. Vários estudos já mostraram que aprendemos melhor dessa forma do que somente assistindo. Teste o programa nos dados também fornecidos no github do autor.\n",
    "\n",
    "Feito isso,  a sua tarefa agora é a seguinte:\n",
    "\n",
    "1. Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna). Baixe os dados no link: [income.csv](https://canvas.instructure.com/courses/1389733/files/68104717/download?verifier=u1l8XB5LcZ51C1MtFrBKJJ9sSPz3f3AOo56Nfk2J&wrap=1). Esse exemplo foi trabalhado em sala de aula. \n",
    "2. Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente.\n",
    "3. O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações? Plote o RSS vs número de iterações.\n",
    "4. Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores.\n",
    "5. O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de parada que é relacionado ao tamanho do gradiente (como no algoritmo apresentado em sala). Plote o tamanho do gradiente vs número de iterações.\n",
    "6. Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?\n",
    "7. Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>26.6588387834389</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.401338</td>\n",
       "      <td>27.306435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.842809</td>\n",
       "      <td>22.132410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.244147</td>\n",
       "      <td>21.169841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.645485</td>\n",
       "      <td>15.192634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.086957</td>\n",
       "      <td>26.398951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          10  26.6588387834389\n",
       "0  10.401338         27.306435\n",
       "1  10.842809         22.132410\n",
       "2  11.244147         21.169841\n",
       "3  11.645485         15.192634\n",
       "4  12.086957         26.398951"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/income.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1\n",
    "Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, np.array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = np.genfromtxt(\"../data/income.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2\n",
    "Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "Step gradient at b = 0.010029093861364507, m = 0.17483245665563385, error = 2648.2381266261386\n",
      "Step gradient at b = 0.16224451219582517, m = 3.0495672429883767, error = 114.65917190116292\n",
      "Step gradient at b = 0.1540919959601231, m = 3.230157419836556, error = 104.70597869157886\n",
      "Step gradient at b = 0.13593224011303429, m = 3.2424867207851698, error = 104.59629197125872\n",
      "Step gradient at b = 0.11715588250239854, m = 3.244306940687362, error = 104.52506735534097\n",
      "Step gradient at b = 0.0983494029715421, m = 3.245470326109695, error = 104.45406006127061\n",
      "Step gradient at b = 0.0795494256035937, m = 3.246592190479218, error = 104.38312085074011\n",
      "Step gradient at b = 0.06075823373709083, m = 3.2477109628793253, error = 104.31224907772851\n",
      "Step gradient at b = 0.041975965992178225, m = 3.248829043600966, error = 104.24144467586197\n",
      "Step gradient at b = 0.023202627044437908, m = 3.249946582789181, error = 104.17070758108825\n",
      "Step gradient at b = 0.0044382132059579285, m = 3.2510635900580587, error = 104.10003772942464\n",
      "Step gradient at b = -0.014317279731615907, m = 3.2521800662448426, error = 104.0294350569496\n",
      "Step gradient at b = -0.0330638560072504, m = 3.2532960116385135, error = 103.95889949980202\n",
      "Step gradient at b = -0.05180151985993397, m = 3.254411426493699, error = 103.88843099418183\n",
      "Step gradient at b = -0.070530275526767, m = 3.2555263110627677, error = 103.8180294763494\n",
      "Step gradient at b = -0.08925012724284324, m = 3.2566406655978346, error = 103.74769488262565\n",
      "Step gradient at b = -0.10796107924124371, m = 3.257754490350886, error = 103.67742714939214\n",
      "Step gradient at b = -0.12666313575303692, m = 3.2588677855737913, error = 103.60722621309095\n",
      "Step gradient at b = -0.14535630100727984, m = 3.2599805515182956, error = 103.53709201022438\n",
      "Step gradient at b = -0.16404057923101878, m = 3.261092788436026, error = 103.4670244773554\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate, itera, verbose=False, period=50):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    \n",
    "    new_error = compute_error_for_line_given_points(new_b, new_m, points)\n",
    "    \n",
    "    if verbose:\n",
    "        if itera % period == 0:\n",
    "            print(\"Step gradient at b = {0}, m = {1}, error = {2}\".format(new_b, new_m, new_error))\n",
    "       \n",
    "    return [new_b, new_m, b_gradient, m_gradient, new_error]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    history = pd.DataFrame(columns=['bias','slope','bias_grad', 'slope_grad','error','iter'])\n",
    "    for i in range(num_iterations):\n",
    "        step_grad = step_gradient(b, m, np.array(points), learning_rate, i, True)\n",
    "        b, m = step_grad[0], step_grad[1]\n",
    "        history = history.append({'bias': step_grad[0], 'slope': step_grad[1], 'bias_grad': step_grad[2],\n",
    "                        'slope_grad': step_grad[3], 'error': step_grad[4], 'iter': i}, ignore_index=True)\n",
    "    return history\n",
    "\n",
    "def run():\n",
    "    points = np.genfromtxt(\"../data/income.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    initial_error = compute_error_for_line_given_points(initial_b, initial_m, points)\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, initial_error))\n",
    "    print(\"Running...\")\n",
    "    \n",
    "    history = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    \n",
    "    final_bias = history['bias'][history.shape[0]-1]\n",
    "    final_slope = history['slope'][history.shape[0]-1]\n",
    "    final_error = compute_error_for_line_given_points(final_bias, final_slope, points)\n",
    "    \n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, final_bias, final_slope, final_error))\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "history = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history['iter'], history['error'] , 'r', label='Model Error') \n",
    "plt.title('Model Error By Iteration') \n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
