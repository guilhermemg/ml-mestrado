{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição\n",
    "\n",
    "Nessa tarefa você vai programar regressão linear simples do zero em python. Para isso, você vai assistir o seguinte vídeo no [youtube](https://www.youtube.com/watch?v=XdM6ER7zTLk) e seguir o passo a passo da implementação. Embora o código esteja disponível no [github](https://github.com/llSourcell/linear_regression_live) do autor. \n",
    "\n",
    "É importante que você digite o código enquanto assiste. Vários estudos já mostraram que aprendemos melhor dessa forma do que somente assistindo. Teste o programa nos dados também fornecidos no github do autor.\n",
    "\n",
    "Feito isso,  a sua tarefa agora é a seguinte:\n",
    "\n",
    "1. Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna). Baixe os dados no link: [income.csv](https://canvas.instructure.com/courses/1389733/files/68104717/download?verifier=u1l8XB5LcZ51C1MtFrBKJJ9sSPz3f3AOo56Nfk2J&wrap=1). Esse exemplo foi trabalhado em sala de aula. \n",
    "2. Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente.\n",
    "3. O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações? Plote o RSS vs número de iterações.\n",
    "4. Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores.\n",
    "5. O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de parada que é relacionado ao tamanho do gradiente (como no algoritmo apresentado em sala). Plote o tamanho do gradiente vs número de iterações.\n",
    "6. Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?\n",
    "7. Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>26.6588387834389</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.401338</td>\n",
       "      <td>27.306435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.842809</td>\n",
       "      <td>22.132410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.244147</td>\n",
       "      <td>21.169841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.645485</td>\n",
       "      <td>15.192634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.086957</td>\n",
       "      <td>26.398951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          10  26.6588387834389\n",
       "0  10.401338         27.306435\n",
       "1  10.842809         22.132410\n",
       "2  11.244147         21.169841\n",
       "3  11.645485         15.192634\n",
       "4  12.086957         26.398951"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/income.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1\n",
    "Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(\"../data/income.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2\n",
    "Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame(columns=['m', 'b', 'error', 'itera'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate, itera, verbose=False, period=50, logs=logs):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    \n",
    "    new_error = compute_error_for_line_given_points(new_b, new_m, points)\n",
    "    \n",
    "    if verbose:\n",
    "        if itera % period == 0:\n",
    "            print(\"Step gradient at b = {0}, m = {1}, error = {2}\".format(new_b, new_m, new_error))\n",
    "    \n",
    "    logs.append([[new_b, new_m, new_error, itera]])\n",
    "    \n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate, i, verbose=True)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(\"../data/income.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    initial_error = compute_error_for_line_given_points(initial_b, initial_m, points)\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, initial_error))\n",
    "    print(\"Running...\")\n",
    "    \n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    final_error = compute_error_for_line_given_points(b, m, points)\n",
    "    \n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, final_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2946.6344970460195\n",
      "Running...\n",
      "Step gradient at b = 0.010029093861364507, m = 0.17483245665563385, error = 2648.2381266261386\n",
      "Step gradient at b = 0.16224451219582517, m = 3.0495672429883767, error = 114.65917190116292\n",
      "Step gradient at b = 0.1540919959601231, m = 3.230157419836556, error = 104.70597869157886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/ml-mestrado/lib/python3.6/site-packages/pandas/core/indexes/api.py:107: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step gradient at b = 0.13593224011303429, m = 3.2424867207851698, error = 104.59629197125872\n",
      "Step gradient at b = 0.11715588250239854, m = 3.244306940687362, error = 104.52506735534097\n",
      "Step gradient at b = 0.0983494029715421, m = 3.245470326109695, error = 104.45406006127061\n",
      "Step gradient at b = 0.0795494256035937, m = 3.246592190479218, error = 104.38312085074011\n",
      "Step gradient at b = 0.06075823373709083, m = 3.2477109628793253, error = 104.31224907772851\n",
      "Step gradient at b = 0.041975965992178225, m = 3.248829043600966, error = 104.24144467586197\n",
      "Step gradient at b = 0.023202627044437908, m = 3.249946582789181, error = 104.17070758108825\n",
      "Step gradient at b = 0.0044382132059579285, m = 3.2510635900580587, error = 104.10003772942464\n",
      "Step gradient at b = -0.014317279731615907, m = 3.2521800662448426, error = 104.0294350569496\n",
      "Step gradient at b = -0.0330638560072504, m = 3.2532960116385135, error = 103.95889949980202\n",
      "Step gradient at b = -0.05180151985993397, m = 3.254411426493699, error = 103.88843099418183\n",
      "Step gradient at b = -0.070530275526767, m = 3.2555263110627677, error = 103.8180294763494\n",
      "Step gradient at b = -0.08925012724284324, m = 3.2566406655978346, error = 103.74769488262565\n",
      "Step gradient at b = -0.10796107924124371, m = 3.257754490350886, error = 103.67742714939214\n",
      "Step gradient at b = -0.12666313575303692, m = 3.2588677855737913, error = 103.60722621309095\n",
      "Step gradient at b = -0.14535630100727984, m = 3.2599805515182956, error = 103.53709201022438\n",
      "Step gradient at b = -0.16404057923101878, m = 3.261092788436026, error = 103.4670244773554\n",
      "After 1000 iterations b = -0.18234255376510086, m = 3.262182267596014, error = 103.39842291729676\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Error By Step Gradient Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>b</th>\n",
       "      <th>error</th>\n",
       "      <th>itera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [m, b, error, itera]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(logs['itera'], logs['error'] , 'r', label='Model Error') \n",
    "plt.title('Model Error By Iteration') \n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
